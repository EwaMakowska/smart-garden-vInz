	W rozdziale tym zbadano możliwości wykorzystania sztucznej inteligencji w ułatwieniu dostępu do kluczowych podstron witryny internetowej albo aplikacji mobilnej przy wykorzystaniu nauczania maszynowego.
	Pomimo, że właściciele witryn internetowych chcą za ich pomocą przekazać dużo istotnych informacji to użytkownicy zazwyczaj szukają bardzo konkretnej wiedzy albo funkcjonalności. Czas poświęcony na znalezienie ich często jest uznawany za czas stracony, a to wywołuje irytacje. Poukładanie witryny w taki sposób, aby użytkownicy intuicyjnie i szybko byli w stanie się w niej odnaleźć może być jednak sztuką. Czy nauczanie maszynowe może w tym pomóc?
	Pierwsza, przetestowana koncepcja zakładała stworzenie modelu który nauczy się rozpoznawać najbardziej popularną podstronę serwisu. Taką stronę można by podpowiadać w odpowiednim miejscu witryny, zadbać o jej poprawne nazewnictwo albo wyróżnić w menu. Najłatwiej to zrobić porównując czasy jakie wszyscy użytkownicy spędzili na danej podstronie. Wskaźnik ten wydaje się najlepiej dobrany do zadania, pozwalając sprawdzić zainteresowanie użytkownika zawartością podstrony serwisu wraz z łatwą możliwością opracowania danych na potrzeby nauczania. Zadanie to można przedstawić jako wyszukiwanie najwyższej wartości z tablicy zawierającej wyrażone w sekundach czasy odpowiednie dla każdej z podstron serwisu.
	Dobrze też zrozumieć, że dla różnego typu serwisów internetowych i aplikacji, najczęściej odwiedzana podstrona może mieć różne znaczenie dla właściciela, oraz można z niej wydobyć różne przydatne informacje. Na przykład w przypadku e-sklepu najpopularniejszą podstroną będzie zapewne strona najpopularniejszego produktu spośród tego co jest sprzedawane. Często może być to nie proste do przewidzenia z wyprzedzeniem, a informacja ta może być wykorzystana w różnoraki sposób, np.: dla lepszego zareklamowania sklepu. Z drugiej strony w przypadku stacjonarnej usługi może to być zakładka „Kontakt”, więc może nie ma sensu utrudniać dostępu do adresu, mapy czy formularza kontaktowego i wyciągnąć je na stronę główną. Taki przypadek można było przewidzieć już wcześniej i uwzględnić na etapie projektowania całego serwisu, i zastosowanie algorytmów nauczania maszynowego wydaje się tu aż bez sensu. Reasumując, to, że na konkretnej podstronie użytkownicy spędzali najwięcej czasu i jest „popularna” wcale nie musi oznaczać czegoś pozytywnego. Być może to właśnie na niej użytkownicy natrafiają na jakieś irytujące, zabierające czas problemy.
	Pierwszym problemem z jakim trzeba się zmierzyć podczas tworzenia modelu nauczania maszynowego jest dostarczenie odpowiednich, oczyszczonych oraz kompletnych danych. Dane o czasach jakie użytkownicy spędzają na twojej stronie można spróbować pobrać z narzędzi takich jak google analitics, SimilarWeb albo używając biblioteki TimeMe.js. Można również w łatwy sposób samodzielnie napisać licznik, który zlicza czas jaki dany użytkownik spędziła na podstronie. Zdecydowanie lepiej podejść do tego od strony użytkownika. Technologie serwerowe nie rejestrują, kiedy użytkownik opuścił stronę, a jedynie kiedy została zaserwowana. Do wykrywania przeładowania można wykorzystać metodę windows.unload lub windows.beforeonload. Niestety każda z nich ma swoje ograniczenie i minusy:

    • nie wykryją zamknięcia przeglądarki,
    • mogą nie zdążyć albo w ogóle mieć zablokowaną możliwość wysłania asynchronicznych żądań, np.: zapisu do bazy danych,
    • mogą zareagować na przeładowanie strony,
    • nie są konsekwentnie obsługiwane w różnych przeglądarkach,
    • w przypadku SPA te technologie mogą w ogóle nie zadziałać.

	Zanim dane dostarczy się do modelu nauczania maszynowego trzeba je oczyścić. Źle przygotowane dane mogą utrudnić, a nawet uniemożliwić skuteczne funkcjonowanie modelu. Na pewno trzeba zadbać o to by nie było żadnych braków, czy wartości nieliczbowych jak „null”. Istotne było również przyjrzenie się czasom mocno odbiegającym od średniej wartości w górę. Użytkownik zapewne pozostawił stronę otwartą i zajął się czymś innym. Zupełne zignorowanie tej wartości również nie jest dobrym pomysłem. Nadal można podejrzewać, że użytkownik chciał znaleźć się na tej stronie, i nie odbiera to jej „popularności”. Z tego powodu zdecydowano się zamienić te wartości na średnią z wszystkich czasów. Przyjrzenie się najniższym wartościom natomiast nie było już aż tak istotne, chociaż przyjmując, że wartości oscylujące wokół kilku sekund wykazują na to, że użytkownik popełnił pomyłkę oraz nie był zainteresowany tą stroną nie zmieni to jej pozycji w rankingu. Nie oznacza to, że duża ilość niskich czasów dla konkretnej podstrony nie jest sama w obie wartościową informacją i nie warto ją wykorzystać w innych analizach.
	Na potrzeby nauczania modelu zdecydowano się natomiast wygenerować dane losowe. Był to zestaw tablic zawierających sześć liczb całkowitych z zakresu od 0 do 10. Pozwoliło to na łatwą manipulację ilością danych, co ma duży wpływ na efekt nauczania, zapewniając przy tym wykonanie zadania. Sześć elementów, co odpowiadało ilości podstron testowego serwisu stworzonego na potrzeby prowadzenia badań. Niestety, ilość danych możliwych do pozyskania z niego była zdecydowanie za mała, aby nadawać się do nauczania maszynowego.
	Do stworzenia etykiet użyta została funkcja max z pakietu numpy. Metoda ta służy dokładnie do odnajdywania najwyższej wartości w podanej tablicy.
	W pierwszej wersji od modelu oczekiwano zwrócenia najwyższej wartości spośród liczb z tablicy. Wykorzystano więc podejście stosowane przy rozwiązywaniu problemów regresji:

    • wyjściowa warstwa modelu posiadała tylko jeden neuron, ponieważ oczekiwano na wyjściu pojedynczej wartości będącej najwyższym czasem,
    • brak zadeklarowania funkcji aktywacyjnej w warstwie wyjściowej,
    • użycie funkcji MSE (Mean Squared Error) jako funkcji straty – funkcja MSE służy do minimalizacji różnicy kwadratowej między rzeczywistą a przewidywaną wartością i najlepiej się sprawdza w rozwiązywaniu problemów regresji.

	Model miał niewiele warstw i neuronów, oraz zadeklarowany callback tensorFlow EarlyStopping ponieważ łatwo ulegał przeuczeniu.
	Jako optymalizer wybrano adam (Adaptive Moment Estimation) z learning_rate ustawionym na domyślną wartość 0.001. Zadaniem optymalizatora jest minimalizacja straty na danych treningowych. Wpływa on na wydajność i skuteczność modelu.
	Adam jest najpopularniejszym optymalizerem wykorzystywanym w nauczaniu maszynowym.  Jest uważany za bardzo skuteczny, dzięki łączeniu w sobie najlepszych cech dwóch innych algorytmów.:

    • zdolność adaptacyjnego uczenia się,
    • możliwość podtrzymywania tylko ostatnich okien gradientów, zamiast akumulowania wszystkich okien.

	Pierwsze z tych dwóch umiejętności polega na przypisywaniu różnym cechą różnych czasów uczenia się. Pozwala to na przypisanie danym rzadko występującym dłuższego czasu nauki. Niestety, problemem tego rozwiązania jest przedwczesna konwergencji, czyli stan, kiedy model osiągnie optymalny poziom i dalsza nauka może doprowadzić do przeuczenia. Pewnym rozwiązaniem tego problemu jest druga z wymienionych cech optymalizera adam. Dzięki temu, że nie są akumulowane wszystkie poprzednie gradienty, ale tylko najbardziej aktualne, system zwraca aktualne informacje do dostosowywania współczynnika uczenia. Efektem tego są bardziej relewantne odpowiedzi modelu oraz wspomniane wcześniej zmniejszenie szans na szybkie osiągnięcie stanu optymalnego.
	Odpowiednie dobranie optymalizera jest kluczowe dla działania modelu, a learning_rates jest najistotniejszym parametrem optymalizatora adam. Jest to krok wykonywany podczas minimalizacji funkcji straty i wpływający bezpośrednio na wagi jakie zostają ustalone dla poszczególnych neuronów podczas nauczania. Zbyt wysokie learning_rates model będzie zwracał niestabilne wyniki, gdyż oscyluje wokół minimum funkcji straty. Znowuż jego zbyt niska wartość może spowodować, że proces nauczania będzie bardzo długi lub utknąć przy niezadowalających wynikach.
	Dodatkowe ustawienia:

    • epoch: 10,
    • batch_size: 10,
    • validation_split: 0.15.

	Callback tensorFlow to bardzo przyjemny mechanizm, pozwalający na wykonanie potrzebnej metody w odpowiedzi na nauczanie modelu. Tą metodą może być na przykład EarlyStopping. Metoda ta może zatrzymać nauczanie w momencie, kiedy poziom straty (loss) albo dokładności (accurency) jest najbardziej optymalny. Zapobiega to osiągnięciu poziomu w którym model zamiast uczyć się, zapamiętuje. Proces ten może spowodować, że system nie radzi sobie z nowymi danymi, a jedynie takimi, z którymi miał styczność podczas procesu nauki.  To również mechanizm callback pozwala na użycie TensorBoard.
	W przypadku omawianego modelu zdecydowano się na obserwacje strat. Ustawiono również dodatkowe parametry: patencie na 3 i restore_best_weight na true. Pierwszy z nich określa w którym dokładnie momencie, po wykryciu potencjalnej możliwości przeuczenia model ma przerwać. Drugi parametr pozwala natomiast na zwrócenie modelu z najlepszymi zaobserwowanymi wynikami poziomu straty, a niekoniecznie dokładnie tych, na których  nauka się zatrzymała.
	Największym problemem modelu było łatwe uleganie przeuczeniu oraz słaba dokładność. Niewielkie zwiększenie ilości epok przez które model miał się uczyć; warstw ukrytych albo neuronów doprowadzało zazwyczaj wskazującego na przeuczenie objawu, czyli zwracania dokładnie takich samych wyników dla każdego możliwego zestawu testowego. Technika EarlyStopping też niekoniecznie okazywała się skuteczna.
	Próbowano również dodać regularyzację l2 na pojedyncze oraz wszystkie warstwy modelu, jednak to też nie pomogło. Technika ta ma zapobiegać przeuczeniu poprzez dodanie do funkcji straty dodatkowego czynnika zależącego od kwadratu wag. Efektem tego wagi układu powinny być mniejsze i bardziej rozproszone, co powinno zapewnić bardziej ogólnikową reprezentację.
	Jeżeli nawet problem przeuczenia dało się opanować i wykres z TensorBoard (wykres 1 i wykres 2) jak i efekty nauczania nie wskazywały na ten problem tak osiągnięcie zadowalającej dokładności wyników przy użyciu modelu dostosowanego do rozwiązywania problemów regresji do zadania odnajdywania najwyższej wartości spośród liczb całkowitych okazał się nie do osiągnięcia. Zwracane wartości, nawet jeżeli były w pobliżu szukanej wartości, to nie dawały możliwości realnego skorzystania z wyników: za bardzo różniły się one zazwyczaj od tych oczekiwanych. Przez to nie pozwalały na dokładne określenie, która z podstron była najdłużej odwiedzana przez użytkowników. Wszelaka próba manipulacji ilością danych, architekturą modelu, długością nauki, parametrami warstw i nauki, a nawet normalizacji danych nie poprawiały sytuacji.
	Wykonanie zadania wymagało więc zmiany w podejściu do nauczania. Zdecydowano się więc na wykorzystanie modelu nauczania przystosowanego do rozwiązywania problemów klasyfikacji.
	Jeżeli chodzi o etykiety to tutaj również użyto metody z pakietu numpy o nazwie argmax. Od metody max różni się tym, że zwraca indeks pod którym znajduje się najwyższa wartość w tablicy, zamiast bezpośrednio wartości.


	Po zmianach architektonicznie model składa się z czterech warstw:

    • warstwa wejściowa: metoda aktywacyjna linear, 64 neurony,
    • warstwa ukryta Dense: metoda aktywacji relu, 32 neuronów,
    • warstwa ukryta Dense: metoda aktywacji relu, 64 neurony,
    • warstwa wyjściowa: metoda aktywacyjna softmax, 50 neuronów.

	Metoda aktywacyjna to funkcjonujący dla każdego pojedynczego neuronu i jeden z kluczowych parametrów nauczania maszynowego. Jej zadaniem jest zmiana danych wejściowych na dane wyjściowe, na podstawie wag neuronów oraz biasów i z wykorzystaniem odpowiedniej funkcji matematycznej.  W uproszczeniu, to dzięki niej neuron decyduje, czy przekazać daną informację do kolejnych warstw, czy nie.
	Użyta w pierwszej warstwie prosta funkcja liniowa jest najprostszą z możliwych metod aktywacyjnych, a za razem jedną z najrzadziej używanych. Siłą zaawansowanych modeli nauczania maszynowego jest ich nieliniowość, co pozwala na dużo bardziej zaawansowaną obróbkę danych. Jednak w niektórych przypadkach, jak na przykład modelach przeznaczonych do rozwiązywania problemów klasyfikacji, zaleca się, aby określone warstwy korzystał z liniowej funkcji aktywacji.
	Kolejne dwie warstwy; ukryte; korzystają z najpopularniejszej metody aktywacji, czyli ReLU (Rectified Linear Unit). Jej największą zaletą jest prostota. Funkcja zwraca podaną wartość, jeżeli wartość jest dodatnia i zero, jeżeli wartość jest ujemna. Ta łatwość w jej obliczeniu przekłada się na szybkość i wzrost wydajność podczas nauczania modelu. Wybierając tę metodę należy mieć jednak na uwadze, że niemożliwość aktywacji dla wartości ujemnych może spowodować zupełne wyłączenie niektórych neuronów, co nazywa się dying ReLU. Rozwiązaniem tego problemu jest zastosowanie jednej ze zmodyfikowanych metod ReLU dopuszczających małe, niezerowe wartości ujemne dla ujemnego argumentu funkcji. Na szczęście w przypadku tworzonego modelu na tym etapie problem dying ReLU nie występował.
	Użycie metoda softmax na wyjściu jest typowym wyborem dla problemów klasyfikacji, szczególnie gdy dotyczy on wielu klas. Jej efektem jest przypisanie prawdopodobieństwa do każdej otrzymanej na wejściu wartości, które łącznie sumuje się do 1. Softmax jest skomplikowaną metodą, polegającą na obliczeniu wartości eksponencjalnej w każdego logitu, następnie dla sumy eksponencjalnej dla wszystkich logitów a na koniec poddaniu wyniku procesowi normalizacji poprzez podzielenie przez siebie wyliczonych wcześniej wartości eksponencjalnych. Logitami w przypadku omawianego modelu nauczania są wartości zwracane przez neurony na ostatniej warstwie ukrytej trafiające do warstwy wyjściowej. Softmax przekształca je na prawdopodobieństwo.
	Jedną z najważniejszych zmian było podmienienie funkcji straty  MSE na categorical_crossentropy. Jej wybór jest jednym z podstawowych podczas tworzenia modelu przeznaczonego do rozwiązywania problemów klasyfikacji. Metoda ta jest szczególnie skuteczna przy zestawie wykluczających się kategorii, czyli kiedy każda cecha może być przypisana tylko i wyłącznie do jednej kategorii. Polega ona na obliczaniu różnicy pomiędzy prawdziwym rozkładem prawdopodobieństwa, a przewidywanym. W przypadku modeli nauczania maszynowego prawdziwy rozkład to etykiety a przewidywany, to dane dostarczane na wejściu. Interpretując rezultat łatwo zauważyć, że im wyższy wynik funkcji, tym niższe prawdopodobieństwo ponieważ „odległość” (różnica) pomiędzy jednym, a drugim rozkładem jest większa.
	Do poprawnego funkcjonowania modelu wystarczyło pozostawić optymalizer adam z learning_rates pozostawionym na wartości domyślnej. W zupełności wystarczyło mu 10 epok. Przy takiej liczbie early_stopping nie musiał zadziałać i na 100% nie dochodziło do przeuczenia. Dla efektywniejszego wykorzystania pamięci komputera podzielono zestawy danych na mniejsze próbki poprzez ustawienie parametru batch_size na 50. Wydzielono również 15% spośród wszystkich dostarczanych modelowi danych do przeznaczenia walidacyjnego, poprzez ustawienie parametru validation_split na 0.15. Jest to bardzo przydatna opcja, pozwalająca lepiej obserwować wyniki jakie otrzymuje model przy danych ustawieniach parametrów, wykrywając dzięki temu na przykład problem z przeuczenie.
Model miał
	Na tym etapie model przyjmuje na wejściu zestaw tablic z sumami czasów dla podstron serwisu, oraz etykiety w postaci indeksu pod którym w tablicy znajduje się najwyższa wartość. Otrzymywane na wyjściu wynik, to prawdopodobieństwo spełnienia warunku dla każdej z cech wprowadzanych na wejściu. Stworzony model zwracał rezultat w postaci notacji naukowej. Dla łatwiejszego odczytania wyników zdecydowano się ją przekonwertować do notacji dziesiętnej. Zrobiono to przy pomocy wbudowanej metody pythona format, która jako pierwszy argument przyjmowała zaokrągloną w dół liczbę w notacji naukowej, a jako drugi, co i jakiej długości ma zwrócić. W przypadku omawianego modelu zdecydowano się ten parametr ustawić na 4f, gdzie f oznacza, że na wyjściu oczekiwane są liczby zmiennoprzecinkowe, a 4 powoduje zaokrąglenie do czwartej liczby po przecinku. Dzięki temu procesowi otrzymane wyniki stały się bardzo łatwe do odczytania: im liczba bliżej 1, tym większe prawdopodobieństwo, że to o tą liczbę chodziło spośród liczb w tablicy.
	Skuteczność modelu okazała się bardzo duża, ma on jednak też i swoje ograniczenia. Użyta do wyszukiwania indeksu, pod którym jest najwyższa wartość w tablicy funkcja z pakietu numpy argmax wskazuje tylko pierwszą wyszukaną najwyższą wartość. Oczywiście, przerobienie modelu w taki sposób, aby wskazywał wszystkie najwyższe wartości jest jak najbardziej możliwe. Jednak nie jest to zadanie proste dla modeli nauczania maszynowego, a takie sytuacje raczej będą rzadkością. Do rozwiązania tego równie dobrze można zastosować tradycyjny algorytm. Znając jedną najwyższą wartość bardzo łatwo sprawdzić, czy nie powtarza się ona w tablicy.
	Kolejnym ograniczeniem modelu jest to, że wymaga on ustawionej na sztywno liczby cech na wejściu. Jest to związane ze strukturą sieci neuronowej - warstwy sieci są połączone ze stałą liczbą neuronów i wag pomiędzy nimi. Jeśli liczba cech wejściowych jest zmienna, sieć neuronowa nie będzie w stanie nauczyć się tych stałych połączeń. W przypadku tworzenia dynamicznego systemu, który ma podpowiadać twórcą stron i aplikacji najbardziej korzystny wygląd interfejsów takie ograniczenie jest już nie małym problemem. Jeden serwis internetowy może mieć sześć podstron, a inny dziesięć. Jeżeli pomyślimy o sklepie internetowym, oferującym dużą gamę produktów to tych podstron może być nawet dobrze ponad 100. Jest to bardzo powszechny problem podczas tworzenia modeli nauczania językowego, a najczęściej stosowane rozwiązania to:

    • rekurencyjne sieci neuronowe (RNN – Reccurent Neutral Network) – klasa sieci neuronowych szczególnie skutecznych w przetwarzaniu danych sekwencyjnych o zmiennej długości czyli na przykład danych z sensorów, dźwięku, obrazu czy właśnie czasu. Dokonano tego, dzięki dodaniu pamięci, co pozwoliło na stworzenie zależności poprzednim i następnym neuronem. Aktualizacja gradientów powoduje jednak, że mają one duże problemy w utrzymaniem długotrwałych powiązań, a wpływ wcześniej podanych danych na model jest znikomy, nawet jeżeli te dane są istotne.
    • sieci o długiej pamięci krótkotrwałej (LSTM – Long Short-Term Memory) – dokładnie rzecz ujmując jest to rozszerzenie RNN mające za zadanie poradzić sobie z problemem zanikającego gradientu. Dokonano tego poprzez wprowadzenie „bramek” kontrolujących przepływ informacji pomiędzy neuronami. Pozwala to na utrzymanie długoterminowych zależności pomiędzy neuronami, tam, gdzie jest to potrzebne i wpływa pozytywnie na efektywność uczenia się.
    • splotowe sieci neuronowe lub konwulsyjne sieci neuronowe (CNN – Convulational Neural Network) – sieci z architekturą mało wrażliwą na kształt wejściowych danych, wykorzystywane z bardzo dużym sukcesem na przykład do analizy obrazów.
    • sprowadzenie ilości cech do stałej wartości, na przykład poprzez poddanie ich wcześniejszym grupowaniom i/lub przekształceniom używając odpowiednio dobranych metod statystycznych. Do tej grupy zalicza się też technika padding (dopełnienie), polegająca na ustaleniu pewnej górnej ilości cech jakie dostaje model, i uzupełnieniem brakujących do tej wartości elementów np.: wartością 0.

	Pierwszą techniką zastosowaną dla rozwiązania omawianego problemu była technika padding. Dla celów testowych pozostawiono sześć losowych liczb całkowitych, a pozostałe pozycje uzupełniono wartością -1. Wybrano tę wartość dlatego, aby mocniej odróżniała się od reszty łatwo wskazując na to, że należy ją zignorować. Rozszerzenie dostarczanych do nauki danych o liczby ujemne, oraz większa ilość cech wymusiły następujące zmiany w architekturze modelu:

    • dostosowanie ilości cech jakie model przyjmuje na wejściu – model musiał przyjmować dużo większą tablicę niż wcześniej,
    • dostosowanie ilości zwracanych danych na wyjściu – model nie mógł obecnie zwracać prawdopodobieństwa tylko dla sześciu pierwszych cech, a musiał brać pod uwagę wszystkiego dostarczone dane, nawet jeżeli były one nieistotne, a prawdopodobieństwo dla nich było równe zero, aby przetworzyć zmienną ilość istotnych danych,
    • zmiana funkcji aktywacyjnej w warstwie wejściowej z relu na linear – relu jest bardzo dobrą funkcją aktywacyjną, pomagająca uczyć się modelowi skomplikowanych rzeczy, jednak dla dobrego działania zamienia wartości ujemne na zero. W przypadku omawianego modelu uniemożliwiło to otrzymywanie logicznych wyników,
    • rezygnacja z konwertowania etykiet metodą to_categorical – metoda ta konwertuje dane na wektory one-hot. Efektem takiego podejścia jest utworzenie dla każdej klasy osobnej kolumny, gdzie wartość 1 oznacza występowanie klasy, a 0 brak występowania, zamiast konkretnych wartości. To binarne podejście wyklucza możliwość stosowania liczb ujemnych,
    • zamiana funkcji straty z categorical_crossentropy na sparse_categorical_crossentropy – implikacją powyżej opisanej zmiany jest też zmiana stosowanej funkcji straty. Categorical_crossentropy wymaga binarnych wektorów i nie zadziała poprawnie z realnymi danymi liczbowymi
    • usunięcie  regularyzację L2 – regularyzacja L2 jest bardzo interesującym narzędziem mającym zapobiegać przeuczeniu, jednak robi to promując rozproszony układ wag z mniejszymi wartościami w modelu. Takie podejście nie ma szans się sprawdzić, gdy dane uzupełnione są sztucznymi wartościami.

	Dużo większym problemem było ustalenie maksymalnej ilości cech, jakie może przyjąć model. Powinna to być najwyższa możliwa liczba podstron, jaka wystąpiła. Jednak liczba ta jest nieokreślona, co więcej, zbyt duża ilość cech mogłaby wpłynąć negatywnie na wydajność model. Z drugiej strony zbyt mała liczba bardzo ograniczyłaby możliwości korzystania z modelu. Ostatecznie, dla testów zdecydowano się na 50 cech, jednak liczba ta jest bardzo dyskusyjna, a małą skuteczność techniki padding w tym rozwiązaniu zadecydowała o próbie wprowadzenia innych rozwiązań.
	Ostatecznie zdecydowano się jeszcze spróbować wdrożyć kolejne rozwiązanie, czyli przekształcić sieć na rekurencyjną. Model typu LSTM został odrzucony, ze względu na to, że wymaga on na wejściu cech w postaci trójwymiarowych danych o kształcie: liczba obserwacji, kroki czasowe, liczba cech, a przygotowane dane nie było prosto skonwertować do tej formy.
	Aby dokonać zmiany na model RNN trzeba było zmienić typ warstwy wejściowej i warstwy ukrytej na SimpleRNN z pakietu tf.keras.layers. SimpleRNN to jedna z najprostszych rodzajów sieci przeznaczonych dla modeli rekurencyjnych. Zapewnia on modyfikowalną pamięć, aktualizującą się każdorazowo, gdy neuron otrzyma nowe dane, co powinno zapewnić bardziej relewantne wyniki.
	W warstwie wejściowej simpleRNN ustawiono parametr return_sequences na true. Pozwala to na dostarczanie danych w takiej formie jak wcześniej dzięki zwracaniu wyników dla każdego kroku czasowego, a nie tylko ostatniej sekwencji wyjściowej. Jednym z efektów tego procesu jest to, że kolejna warstwa simpleRNN otrzymuje tablice 3D z układem liczba próbek, kroki czasowe, liczba jednostek.
	Zmiany te pozwoliły na dostosowanie paramatru input_shape na tablice z dwoma elementami: none i 1. Pierwszy element tablicy odnosi się do tego, że długość sekwencji może być dowolna, a drugi wskazuję na liczbę cech w każdym kroku. Takie ustawienie w praktyce pozwala na przesyłanie dowolnej ilości cech w sekwencji.
	Na zakończenie model wymagał jeszcze drobnych zmian w ilości warstw oraz przywrócenia regulizatora L2 w celu utrzymania zadowalającego poziomu wyników.
	Ciekawym podejściem do tego modelu mogłoby być zestawienie dostarczanych danych z innym zestawem danych. Rzecz jasna, aby to posunięcie miało sens te dane muszą być z sobą skorelowane w jakiś konkretny sposób. Na pierwszą myśl przychodzi ilość odwiedzin. Można ją również brać za wskaźnik popularności strony jak i czas spędzony na stronie.
	Zdobycie danych ilości odwiedzin również jest dość prostym zadaniem. Można skorzystać z gotowych narzędzi jak Google Analitics i bibliotek zewnętrznych lub napisać to samodzielnie. Do zrobienia go nie potrzeba żadnej specjalnej techniki. Chcąc napisać licznik odwiedzin w javaScript wystarczy zrobić prostą inkrementację. Wykrywanie ładowania strony od backendu wydaje się jednak lepszym rozwiązaniem. Najlepiej oczywiście jest użyć danych sesyjnych. Cokolwiek jednak się zrobi taki system będzie bardzo łatwo ulegał manipulacją, a każde odświeżenie strony będzie traktowane jako wejście na nią i zwiększy liczbę odwiedzin o 1. Może to bardzo mocno wpłynąć na wynik. Teoretycznie dane sesyjne dają szanse na wykrycie takiego zachowania poprzez możliwość obserwacji który użytkownik zażądał zaserwowania strony. W praktyce powodów dla których użytkownik potrzebował kilka razy odświeżyć stronę może być wiele. Użytkownik może mieć problem z otrzymaniem zawartości, albo wykonaniem jakiejś akcji na stronie. Mógł też kliknąć przypadkowo kilka razy w ikonę odświeżania. Na pewno mając możliwość zestawienia ilości odwiedzin, wraz z czasem jaki konkretny użytkownik spędził na tej konkretnej podstronie w konkretnym czasie daje możliwość lepszego opracowania danych. Wykrycia schematu częstych odświeżeń występujących w krótkim czasie i wywołanych przez jednego użytkownika dale możliwość scalenia występujących wtedy odwiedzin w   jedno, dłuższe.
	Jednak, tak samo jak i wcześniej, to że licznik odwiedzin dla konkretnej podstrony jest najwyższy, wcale nie musi świadczyć o popularności strony, a w powiązaniu z krótkim czasem jaki użytkownik spędził na podstronie może nawet świadczyć o tym, że użytkownicy wchodzą tutaj przez przypadek, albo muszą wejść na tę stronę, aby przedostać się na tę, która ich dokładnie interesuje.
	Bardzo dużym problemem może również okazać się to, że korelacja pomiędzy liczbą odwiedzin, a czasem spędzonym na stronie może być bardzo słaba. To, że podstrona ma niewielką ilość odwiedzin, ale długi czas, jaki użytkownicy na niej spędzają może świadczyć na przykład, o tym, że zawartość jej jest przeznaczona dla ograniczonego profilu odbiorcy, mocno profesjonalna, albo kierowana do odpowiednich grup. Jednak w kontekście wykrywania najbardziej popularnej podstrony nie ma to znaczenia. Pomimo małej ilości odwiedzin, użytkownicy nadal spędzają na niej najwięcej czasu chcąc zapoznać się z treścią, co czyni ją najpopularniejszą podstroną tego serwisu.
	Niestety, poszukując badań na temat związku pomiędzy czasem, jaki użytkownicy spędzają na stronie internetowej a ilością odwiedzin tej strony udało się dotrzeć do niewielkiej ilości wartościowego kontentu. Ilość wzmianek na ten temat jest bardzo i chociaż wstępna analiza wydaje się potwierdzać fakt istnienia korelacji pomiędzy omawianymi zmiennymi [1], to brak większej ilości potwierdzających to badań.
	Przed użyciem kolejnego zestawu danych można dokonać samodzielnej oceny ich korelacji. Są trzy najpopularniejsze metody sprawdzania korelacji pomiędzy zestawami danych:

    • współczynnik korelacji Pearsona – dzięki niemu można poznać liniową zależność pomiędzy dwoma cechami. W przypadku idealnie dodatniej korelacji (jeżeli wartość pierwszej cechy rośnie, wartość drugiej też) wynik wynosi idealnie 1, -1 jeżeli korelacja jest negatywna (gdy wartość jednej cechy wzrasta, drugiej maleje, oraz 0 przy braku korelacji;
    • współczynnik korelacji Spearmana – metoda ta mierzy zależności pomiędzy rankingami obu zestawów danych, a nie bezpośrednią zależność.  Jej wynik interpretuje się tak samo, jak w przypadku współczynnika korelacji Pearsona, jednak nadaje się również do wykrywania zależności nieliniowych,
    • współczynnik korelacji Kendalla – metoda służy do opisu powiązań pomiędzy dwiema zmiennymi porządkowymi. Tak samo jak współczynnik korelacji Spearsmana można go wykorzystywać do badania zbieżności nieliniowych. Interpretacja wyników wygląda tak samo jak w przypadku wyżej wymienionych metod.

	Ponieważ przedstawione badania skupiają się na najwyższych wartościach współczynnik korelacji Persona powinien nadać się najlepiej, podpowiadając, czy częstotliwość odwiedzin strony wpływa na dodatnio, ujemnie, czy obojętnie na czas, jaki użytkownicy na niej spędzają.
	Współczynnik korelacji Pearsona  oblicza się na podstawie dwóch składników. Kowariancja to wartość zmiany danych względem siebie, a odchylenie standardowe mierzy jak pojedyncza zmienna różni się od swojej średniej. W python można ją bardzo łatwo zaimplementować z biblioteki pandas lub numpy. Na potrzeby badań wykorzystano metodę numpy. Ma to pozwolić na odsianie nieskorelowanych danych, pozwalając na lepszą analizę przypadków skorelowanych przypadków.
	Aby przystosować model do analizy równoległego zestawu cech, potrzeba było dokonać modyfikacji. Stworzony model przyjmuje tylko jeden zestaw cech, na których się uczy, porównując wyniki z etykietami.
	Wzbogacić modelu o kolejny zestaw równoległych cech odbyło się poprzez rozbudowę tablicy z podstawowym zestawem cech; tablicy czasów; o kolejny wymiar, zawierający informacje o ilości wejść na daną podstronę. Ta wielowymiarowa tablica oczywiście została przekonwertowana na tablice numpy oraz poddana procesowi skalowania przy pomocy narzędzia MaxScalar.
	Model nie używa tych danych bezpośrednio do uzyskania wyników, ale dzięki analizie korelacji powinien osiągać lepsze i bardziej adekwatne wyniki.
	Oczywiście, zastanawiając się nad innymi, mocniej skorelowanymi z czasem jaki użytkownicy spędzają na konkretnej podstronie danymi wyznaczono sześć czynników:

    • zawartość strony – dłuższy tekst albo strona hostująca pliki audio oraz wideo mają znaczący wpływ na czas jaki użytkownik spędza na stronie;
    • interaktywność – formularze do wypełnienia, ale też gry, quizy, zabawy edukacyjne -  to wszystko wpływa na czas jaki użytkownicy spędzają na stronie;
    • łatwość nawigacji – łatwa w nawigowaniu strona, na której użytkownik szybko odnajdzie lub wykona to co potrzebuje również mocno wpływa na czas jaki użytkownik spędził na stronie;
    • dostępność mobilna – obecnie już bardzo mały procent stron nie jest dostosowany na urządzenia mobilne, jednak jeżeli użytkownik trafi na taką stronę, może szybko się zniechęcić do jej dalszego przeglądania;
    • reklamy – ilość i rodzaj reklam na stronie również może wpłynąć na czas jaki użytkownicy na niej spędzają.

	Uzyskanie takich danych jak długość tekstów znajdujących się na badanej stronie, ilość plików audio i wideo oraz występowanie formularza wraz z ilością zawartych w nich pól do wypełnienia oraz wyskakujących na start strony popupów, mogących być reklamami może być wykonane bezpośrednio z poziomu pythona. Zastosowanie do tego metody web-scraping powinno przynieść zadowalające rezultaty. Technika ta to proces ekstrakcji pewnych danych ze strony internetowej. Za pomocą biblioteki request z metodą get, która jako parametr przyjmuje adres url strony można pobrać zawartość dokumentu. Następnie wydobyć interesujące dane używając biblioteki BeautifulSoup z pakietu bs4. Metoda ta pozwala na przekształcenie HTML w specjalny obiekt umożliwiający bardziej efektywne wyszukiwanie. Wywoływane na tym obiekcie metody w łatwy sposób pozwalają na przykład na szybkie pobranie i analizę zawartości konkretnych tagów.
	Niestety, technika ta sama w sobie jest mocno ograniczona i nie zapewni poprawnej analizy na przykład w przypadku single page aplication, albo jeżeli z innego powodu większa ilość kodu jest dostarczana dynamicznie. Aby obejść ten problem można wyhostować najpierw stronę na silniku Selenium lub Papettery, a następnie  pobrać wygenerowany kod, uzupełniony już o cały dostępny w danym momencie HTML. W przypadku plików audio i video użycie jednego z tych driver-ów pozwoli na uzyskanie danych o długości konkretnych plików.
	Analizując temat dostępności stron internetowych na urządzenia mobilne, biorąc pod uwagę:

    • raport zamieszczony przez DataRaport nt.: ruchu w internecie za rok 2024 [2], z którego wynika, że 69% ruchu w internecie odbywa się z udziałem urządzeń mobilnych. DataReportal to internetowa biblioteka raportów, która oferuje wiarygodne i sprawdzone dokumenty zawierające interesujące dane, szczególnie dotyczące korzystania z internetu, urządzeń mobilnych, aplikacji, mediów społecznościowych i e-commerce na całym świecie.
    • Ilość nieaktywnych stron internetowych  [3], która, według raportu Netcraft September 2024 Web Server Survey wynosi 82% spośród 1,119,023,272 wyhostowanych stron internetowych. Analiza tego tematu wykracza poza temat pracy, ale ciężko nie podejrzewać, że duża część z tych stron została porzucona na rzecz nowocześniejszych witryn stworzonych po to, aby dostosować je do współczesnych wymagań rynku, wśród których jest responsywność. Netcraft to Brytyjska firma uznawana za autorytet w dziedzinie analizy udziału w rynku serwerów internetowych, systemów operacyjnych, dostawców hostingu, ISP, transakcji szyfrowanych, e-commerce, języków skryptowych i technologii treści w internecie.
    • Wysoki poziom wykorzystywania przez biznes internetu do promowania swoich usług i produktów oraz znajdywania klientów.
    • Uwzględnianie dostosowania layout aplikacji do wersji mobilnej już na etapie planowania strony internetowej
    • Promowane przez firmy; zajmujące się narzędziami wspierającymi tworzenie layout takie jak np. Bootstrap; podejścia architektonicznego mobile first;

	można założyć, że witryn internetowych, nie funkcjonujących dobrze lub wcale na urządzeniach mobilnych jest naprawdę niewielki procent, który wydaje się mało istotny do analizy.
	Z drugiej strony; jak wskazują raporty; zachowania użytkowników, przeglądających internet poprzez urządzenia mobilne oraz stacjonarne różnią się od siebie. Co bardzo interesujące; za równo w kontekście tworzonego modelu, jak i większego udziału urządzeń mobilnych w ruchu w internecie; użytkownicy przeglądają średnio więcej stron internetowych i poświęcają każdej z nich średnio więcej czasu niż korzystając z urządzeń mobilnych [4, 5]. Badania te zostały przeprowadzone przez renomowaną firmą zajmującą się analizą danych i marketingiem cyfrowym Semrush przy użyciu narzędzia Traffics Analitics na ponad 100 stronach.
	Biorąc pod uwagę wyżej wymienione powody, wzbogacenie badań o opisany wyżej aspekt wydaje się wnieść dużo do omawianego modelu.
	Informacje o tym, z jakiego typu urządzenia wyszedł request można uzyskać z nagłówka user-agent żądania HTTP. Zawarte tam informacje o typie i wersji przeglądarki pozwolą na przypisanie urządzenia do jednego z dwóch typów:

    • urządzenia mobilne wraz z tabletami – 0,
    • urządzenie stacjonarne, w tym laptopy – 1.

	Nagłówek user-agent można pobrać przy pomocy pythona używając biblioteki flask, albo skorzystać z przeglądarkowego obiektu navigator na przykład w javaScript. Oczywiście, informacja zawarta w nagłówku user-agent nie zawsze będzie adekwatna do rzeczywistości. Może być ona łatwo zmieniona przez użytkownika, albo oprogramowanie, a dodatkowo niektóre przeglądarki emulują określone typy, utrudniając otrzymanie tej informacji. Jednak są to raczej sytuacje marginalne i zamierzone, a uzyskana informacja w większości przypadków powinna być prawdziwa. Takie informacje można również uzyskać za pośrednictwem googleAnalitics, albo innych narzędzi analizy stron www.
	Zupełnie inaczej wygląda sprawa z oceną nawigacji na stronie. Stwierdzenie „dobra”, „zła”, czy „średnia” nawigacja są bardzo mało konkretne, a co jeszcze bardziej utrudnia sprawę ta ocena jest bardzo subiektywna. To, co dla jednej osoby będzie bardzo dobrą nawigacją, druga osoba oceni negatywnie. Również zebranie informacji na ten temat wydaje się tematem trudnym, a zdobycie danych nie zabarwionych subiektywną opinią użytkownika wydaje się procesem skomplikowanym; o ile nie niemożliwym do wykonania.
	Oczywiście, można by spróbować wyznaczyć elementy niezbędne dla dobrego menu, jak czytelność, intuicyjność, sprawne i szybkie działanie, niezawodność, lokalizacja i tym podobne, a następnie wykorzystać nauczanie maszynowe, do analizy nawigacji witryny. Jednak jest to już temat na kolejne badania oraz oddzielny rozdział.
	Przewidywanie najbardziej popularnej strony w serwisie wykonane przy pomocy głębokiego nauczania maszynowego wydaje się bez sensu. Bardzo łatwo i bardziej wydajnie można to samo osiągnąć konwencjonalnymi algorytmami. Wystarczy wyszukać najwyższą wartość spośród danych wartości. Niestety, w rzeczywistości sprawa może wcale nie musi być taka prosta. To wiele czynników decyduje o tym, czy najchętniej odwiedzana lub najdłużej przeglądana strona internetowa jest rzeczywiście tą najpopularniejsza. Możliwości nauczania maszynowego pozwalają na uchwycenie wpływów jednych czynników na inne często niedostrzegalnych dla nas. Dzięki temu wyniki proponowane przez model sztucznej inteligencji powinny być bardziej adekwatne, a dzięki temu przyczynić się do stworzenia lepszego interfejsu aplikacji i zwiększyć zadowolenie użytkowników.












